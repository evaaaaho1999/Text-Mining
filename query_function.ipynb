{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_query_function.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evaaaaho1999/Text-Mining/blob/main/query_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhkB53Pmzy_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1da4b66-b6c4-48a2-b902-16b973091e67"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import gdown\n",
        "nltk.download('punkt')\n",
        "def get_df():\n",
        "    # df_query\n",
        "    url = \"https://drive.google.com/u/0/uc?id=1-24aWX9oqnhYm65onxV5Zm4PbXppvtGD&export=download\"\n",
        "    gdown.download(url, \"news_en.csv\", quiet=True)\n",
        "    return pd.read_csv(\"news_en.csv\")\n",
        "def get_doc_all(df_query):\n",
        "    # doc_all\n",
        "    doc_all={}\n",
        "    for i in range(len(df_query)):\n",
        "        tokens = nltk.word_tokenize(df_query.news[i])\n",
        "        token_filtered = [w.lower() for w in tokens if w.isalpha()]\n",
        "        doc_all[i]=token_filtered\n",
        "    return doc_all\n",
        "# Declare all function \n",
        "# create tf function\n",
        "def tf(term, token_doc):\n",
        "    tf = token_doc.count(term)/len(token_doc)\n",
        "    return tf\n",
        "\n",
        "# create function to calculate how many doc contain the term \n",
        "def numDocsContaining(word, token_doclist):\n",
        "    doccount = 0\n",
        "    for doc_token in token_doclist:\n",
        "        if doc_token.count(word) > 0:\n",
        "            doccount +=1\n",
        "    return doccount\n",
        "  \n",
        "import math\n",
        "# create function to calculate  Inverse Document Frequency in doclist - this list of all documents\n",
        "def idf(word, token_doclist):\n",
        "    n = len(token_doclist)\n",
        "    df = numDocsContaining(word, token_doclist)\n",
        "    return math.log10(n/df)\n",
        "\n",
        "#define a function to do cosine normalization a data dictionary\n",
        "def cos_norm(dic): # dic is distionary data structure\n",
        "  import numpy as np\n",
        "  dic_norm={}\n",
        "  factor=1.0/np.sqrt(sum([np.square(i) for i in dic.values()]))\n",
        "  for k in dic:\n",
        "    dic_norm[k] = dic[k]*factor\n",
        "  return dic_norm\n",
        "\n",
        "#create function to calculate normalize tfidf \n",
        "def compute_tfidf(token_doc,bag_words_idf):\n",
        "  tfidf_doc={}\n",
        "  for word in set(token_doc):\n",
        "    tfidf_doc[word]= tf(word,token_doc) * bag_words_idf[word]   \n",
        "  tfidf_norm = cos_norm(tfidf_doc)\n",
        "  return tfidf_norm\n",
        "\n",
        "# create normalize term frequency\n",
        "def tf_norm(token_doc):\n",
        "  tf_norm={}\n",
        "  for term in token_doc:\n",
        "    tf = token_doc.count(term)/len(token_doc)\n",
        "    tf_norm[term]=tf\n",
        "  tf_max = max(tf_norm.values())\n",
        "  for term, value in tf_norm.items():\n",
        "    tf_norm[term]= 0.5 + 0.5*value/tf_max\n",
        "  return tf_norm\n",
        "  \n",
        "def compute_tfidf_query(query_token,bag_words_idf):\n",
        "  tfidf_query={}\n",
        "  tf_norm_query = tf_norm(query_token)\n",
        "  for term, value in tf_norm_query.items():\n",
        "    tfidf_query[term]=value*bag_words_idf[term]   \n",
        "  return tfidf_query\n",
        "def add_tfidf_of_query(tfidf_query):\n",
        "    tfidf[\"query\"]\n",
        "def get_tfidf_query(input_query,bag_words,bag_words_idf):\n",
        "    # tfidf_query,query_token\n",
        "    query=str(input_query)\n",
        "    query=query.lower()\n",
        "    query_token_raw= nltk.word_tokenize(query)\n",
        "    query_token = [term for term in query_token_raw if term in bag_words]\n",
        "    if len(query_token) >0 :\n",
        "        tfidf_query =compute_tfidf_query(query_token,bag_words_idf) #calculate tfidf for query text\n",
        "        return tfidf_query,query_token\n",
        "    else:  \n",
        "        return 0\n",
        "def web_crawer():\n",
        "    import bs4\n",
        "    from bs4 import BeautifulSoup as soup\n",
        "    from urllib.request import urlopen\n",
        "\n",
        "    news_url=\"https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB?hl=en-US\"\n",
        "    Client=urlopen(news_url)\n",
        "    xml_page=Client.read()\n",
        "    Client.close()\n",
        "\n",
        "    soup_page=soup(xml_page,\"xml\")\n",
        "    news_list=soup_page.findAll(\"item\")\n",
        "    # Print news title, url and publish date\n",
        "    news_title=[]\n",
        "    website=[]\n",
        "    today_news=[]\n",
        "    i=0\n",
        "    for news in news_list:\n",
        "        news_title.append(news.title.text)\n",
        "        website.append(news.link.text)\n",
        "        i+=1\n",
        "    \n",
        "    return news_title,website\n",
        "def query_and_answer(input_query=\"trump\"):\n",
        "    news_title,website = web_crawer()\n",
        "    df_query = get_df()\n",
        "    doc_all = get_doc_all(df_query)\n",
        "    \n",
        "    #create bag words\n",
        "    bag_words =[] # declare bag_words is a list\n",
        "    for doc in doc_all.keys():\n",
        "        bag_words += doc_all[doc]\n",
        "    bag_words=set(bag_words)\n",
        "\n",
        "\n",
        "    bag_words_idf={} # declare \"bag_words_idf\" data structure is dictionary \n",
        "    bag_words_len = len(bag_words)\n",
        "    bag_word_10 = round(bag_words_len/10,0)\n",
        "\n",
        "    i=0\n",
        "    for word in bag_words:\n",
        "        i+=1\n",
        "        bag_words_idf[word]= idf(word,doc_all.values())\n",
        "    tfidf={} \n",
        "    for doc in doc_all.keys():\n",
        "        tfidf[doc]= compute_tfidf(doc_all[doc],bag_words_idf)\n",
        "\n",
        "    tfidf_query,query_token = get_tfidf_query(input_query,bag_words,bag_words_idf)\n",
        "\n",
        "    # add tfidf of query text to tfidf of all doc and convert to dataframe\n",
        "    tfidf[\"query\"]=tfidf_query\n",
        "\n",
        "    import pandas as pd\n",
        "    tfidf_df = pd.DataFrame(tfidf).transpose()\n",
        "    tfidf_df= tfidf_df.fillna(0) # replace all NaN by zero\n",
        "\n",
        "    from scipy.spatial.distance import cosine\n",
        "    cosine_sim ={}\n",
        "    for row in tfidf_df.index:\n",
        "        if row != \"query\":\n",
        "            cosine_sim[row]= 1-cosine(tfidf_df.loc[row],tfidf_df.loc[\"query\"])\n",
        "\n",
        "    # the top 10 relevant document\n",
        "    if len(query_token) >0 :\n",
        "        cosine_sim_top10 = dict(sorted(cosine_sim.items(), key=lambda item: item[1],reverse=True)[:10])\n",
        "        # print(cosine_sim_top10)\n",
        "\n",
        "    returns=list(cosine_sim_top10.keys())[0]\n",
        "\n",
        "    return (news_title[returns],website[returns])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94xLqMCO4Sl2"
      },
      "source": [
        "tp = query_and_answer(\"trump\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lPHA3s4hL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3e476d67-fa5a-479a-8b39-82dea06b81a3"
      },
      "source": [
        "tp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Dallas County Reports Record 328 COVID-19 Cases Friday, Adds 3 Deaths - NBC 5 Dallas-Fort Worth',\n",
              " 'https://news.google.com/__i/rss/rd/articles/CBMidWh0dHBzOi8vd3d3Lm5iY2Rmdy5jb20vbmV3cy9jb3JvbmF2aXJ1cy9kYWxsYXMtY291bnR5LXJlcG9ydHMtcmVjb3JkLTMyOC1jb3ZpZC0xOS1jYXNlcy1mcmlkYXktYWRkcy0zLWRlYXRocy8yMzg3OTAwL9IBeWh0dHBzOi8vd3d3Lm5iY2Rmdy5jb20vbmV3cy9jb3JvbmF2aXJ1cy9kYWxsYXMtY291bnR5LXJlcG9ydHMtcmVjb3JkLTMyOC1jb3ZpZC0xOS1jYXNlcy1mcmlkYXktYWRkcy0zLWRlYXRocy8yMzg3OTAwLz9hbXA?oc=5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9btEodgD5npX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}