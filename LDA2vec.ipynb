{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmtDaoETSY2vGIoGFNTZi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evaaaaho1999/Text-Mining/blob/main/LDA2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSZq4hfKx1mN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWZCJKqHJm6"
      },
      "source": [
        "## LDA & Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JypA_p_13aW1"
      },
      "source": [
        "b=np.load('/content/gdrive/MyDrive/Statistical computing Final project/files/tokens_all.npy',allow_pickle=True)\n",
        "b=b.tolist()\n",
        "b_train=b[0:41157]\n",
        "c=list(b_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGtXQ2_iOMYH"
      },
      "source": [
        "#透過gensim以text_data建立字典\n",
        "from gensim import corpora\n",
        "dictionary =corpora.Dictionary(b_train)\n",
        "#語料庫\n",
        "corpus = [dictionary.doc2bow(token) for token in b_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsKpbr1KA-dI"
      },
      "source": [
        "import gensim\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "num_topics = 150\n",
        "#A multicore approach to decrease training time\n",
        "LDAmodel = LdaMulticore(corpus=corpus,\n",
        "              id2word=dictionary,\n",
        "              num_topics=num_topics,\n",
        "              workers=4,\n",
        "              chunksize=2000,\n",
        "              passes=7,\n",
        "              alpha='asymmetric')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDlTxm6jCXTN"
      },
      "source": [
        "def document_to_lda_features(lda_model, document):\n",
        "    \"\"\" Transforms a bag of words document to features.\n",
        "    It returns the proportion of how much each topic was\n",
        "    present in the document.\n",
        "    \"\"\"\n",
        "    topic_importances = LDAmodel.get_document_topics(document, minimum_probability=0)\n",
        "    topic_importances = np.array(topic_importances)\n",
        "    return topic_importances[:,1]\n",
        "\n",
        "train['lda_features'] = list(map(lambda doc:\n",
        "                                      document_to_lda_features(LDAmodel, doc),\n",
        "                                      corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idUQPVVrF8tT",
        "outputId": "9976431e-01b1-4098-c31c-a6b5b363c440"
      },
      "source": [
        "sentences = []\n",
        "for sentence_group in b:\n",
        "    sentences.extend(sentence_group)\n",
        "\n",
        "print(\"Number of sentences: {}.\".format(len(sentences)))\n",
        "print(\"Number of texts: {}.\".format(len(train)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 818236.\n",
            "Number of texts: 41157.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVm1I09GHF2G"
      },
      "source": [
        "# Set values for various parameters\n",
        "num_features = 200    # Word vector dimensionality\n",
        "min_word_count = 3    # Minimum word count\n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 6           # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model\n",
        "W2Vmodel = Word2Vec(sentences=sentences,\n",
        "                    sg=1,\n",
        "                    hs=0,\n",
        "                    workers=num_workers,\n",
        "                    size=num_features,\n",
        "                    min_count=min_word_count,\n",
        "                    window=context,\n",
        "                    sample=downsampling,\n",
        "                    negative=5,\n",
        "                    iter=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECpv9nV9ObJu",
        "outputId": "29829c60-10f7-49e8-e5c6-219fa5dfe334"
      },
      "source": [
        "train2=train['lemma_text'].dropna().to_frame().reset_index()\n",
        "train2=train2.drop(columns=['index'])\n",
        "b_train2 = b_train[:]\n",
        "b_train2.remove(b_train[33122])\n",
        "b_train2.remove(b_train[33123])\n",
        "b_train2.remove(b_train[33124])\n",
        "b_train2.remove(b_train[39206])\n",
        "b_train2.remove(b_train[39207])\n",
        "b_train2.remove(b_train[39208])\n",
        "c=list(b_train2)\n",
        "len(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41151"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCbrGUDpSU2I",
        "outputId": "9aa94478-0641-491b-8906-413c4ba251f7"
      },
      "source": [
        "type(train['lemma_text'][1].split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shKMnEHdIQPi",
        "outputId": "4b58b7ca-d959-4699-9ada-e1e24f561270"
      },
      "source": [
        "def get_w2v_features(w2v_model, sentence_group):\n",
        "    \"\"\" Transform a sentence_group (containing multiple lists\n",
        "    of words) into a feature vector. It averages out all the\n",
        "    word vectors of the sentence_group.\n",
        "    \"\"\"\n",
        "    words = sentence_group\n",
        "    # words = np.concatenate([np.expand_dims(i,axis=0) for i in sentence_group])# words in text\n",
        "    index2word_set = set(w2v_model.wv.vocab.keys())  # words known to model\n",
        "    \n",
        "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
        "    \n",
        "    # Initialize a counter for number of words in a review\n",
        "    nwords = 0\n",
        "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            featureVec = np.add(featureVec, w2v_model[word])\n",
        "            nwords += 1.\n",
        "\n",
        "    # Divide the result by the number of words to get the average\n",
        "    if nwords > 0:\n",
        "        featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "train['w2v_features'] = list(map(lambda sen_group:\n",
        "                                      get_w2v_features(W2Vmodel, sen_group),\n",
        "                                      train['lemma_text']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnmlm2ckIQNT"
      },
      "source": [
        "train.to_csv(r'/content/gdrive/MyDrive/Statistical computing Final project/files/train_with_feature.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzs4X8DwUwX6"
      },
      "source": [
        "train2=pd.read_csv(\"/content/gdrive/MyDrive/Statistical computing Final project/files/train_with_feature.csv\",encoding='latin1')\n",
        "train2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM77nodbnlcu"
      },
      "source": [
        "# since train_data['lda_features'] and train_data['w2v_features'] don't have the needed shape and type yet,\n",
        "# we first have to transform every entry\n",
        "X_train_lda = np.array(list(map(np.array, train2.lda_features)))\n",
        "X_train_w2v = np.array(list(map(np.array, train2.w2v_features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyHyKeyhpyMw",
        "outputId": "77d1cc32-bf3c-4c81-9490-34a2ebec6397"
      },
      "source": [
        "y_train=train2['sentiment'].dropna().astype(str)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "labelencoder.fit(y_train)\n",
        "# labelencoder.fit(train['sentiment'])\n",
        "y_train = labelencoder.transform(y_train)\n",
        "len(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41151"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuds9K8jaCJv"
      },
      "source": [
        "X_train_lda=np.delete(X_train_lda, 33122, 0)\n",
        "X_train_lda=np.delete(X_train_lda, 33123, 0)\n",
        "X_train_lda=np.delete(X_train_lda, 33124, 0)\n",
        "X_train_lda=np.delete(X_train_lda, 39206, 0)\n",
        "X_train_lda=np.delete(X_train_lda, 39207, 0)\n",
        "X_train_lda=np.delete(X_train_lda, 39208, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 33122, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 33123, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 33124, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 39206, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 39207, 0)\n",
        "X_train_w2v=np.delete(X_train_w2v, 39208, 0)\n",
        "X_train_combined = np.append(X_train_lda, X_train_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6Ru1HJIqAk-"
      },
      "source": [
        "def get_cross_validated_model(model, param_grid, X, y, nr_folds=5):\n",
        "    \"\"\" Trains a model by doing a grid search combined with cross validation.\n",
        "    args:\n",
        "        model: your model\n",
        "        param_grid: dict of parameter values for the grid search\n",
        "    returns:\n",
        "        Model trained on entire dataset with hyperparameters chosen from best results in the grid search.\n",
        "    \"\"\"\n",
        "    # train the model (since the evaluation is based on the logloss, we'll use neg_log_loss here)\n",
        "    grid_cv = GridSearchCV(model, param_grid=param_grid, scoring='neg_log_loss', cv=nr_folds, n_jobs=-1, verbose=True)\n",
        "    best_model = grid_cv.fit(X, y)\n",
        "    # show top models with parameter values\n",
        "    result_df = pd.DataFrame(best_model.cv_results_)\n",
        "    show_columns = ['mean_test_score', 'mean_train_score', 'rank_test_score']\n",
        "    for col in result_df.columns:\n",
        "        if col.startswith('param_'):\n",
        "            show_columns.append(col)\n",
        "    display(result_df[show_columns].sort_values(by='rank_test_score').head())\n",
        "    return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUKkqgP8pPa9"
      },
      "source": [
        "# store all models in a dictionary\n",
        "models = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeLROyeicLmO"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf=LogisticRegression(fit_intercept=True,C=1e15)\n",
        "clf.fit(aaa,y_train)\n",
        "print(clf.intercept_,clf.coef_)\n",
        "#與第一種方法相比\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIK_iPkMbxTU"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "param_grid = {'penalty': ['none', 'l2']}\n",
        "grid_cv =GridSearchCV(lr, param_grid=param_grid, scoring='neg_log_loss', cv=5, n_jobs=-1, verbose=True)\n",
        "best_model = grid_cv.fit(X_train_lda, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vOlpvtjpiIb"
      },
      "source": [
        "# LDA features only\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "lr = LogisticRegression()\n",
        "param_grid = {'penalty': ['none', 'l2']}\n",
        "\n",
        "best_lr_lda = get_cross_validated_model(lr, param_grid, X_train_lda, y_train)\n",
        "\n",
        "models['best_lr_lda'] = best_lr_lda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-pi-Pxerp49"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}