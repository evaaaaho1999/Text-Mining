{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_clustering_sample_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kosqpu72wAou",
        "colab_type": "text"
      },
      "source": [
        "#Text clustering -sample code\n",
        "** Prepage datasets**\n",
        "Download dataset movie_data_60.csv from google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyTYrEvGB_k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount google drive to google colab virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive =\"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2qw3NQHgJzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from google drive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1o0I46KhBjkW2tdxD-k9NwkhrtQbI6gNF\"\n",
        "gdown.download(url, mydrive+\"movies_genre_60.csv\", quiet=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbT19rrfEh-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data to pandas data frame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "doc_data = pd.read_csv(mydrive+'movies_genre_60.csv', sep=\",\", header=0 )\n",
        "movie_titles = doc_data['Title'].tolist()  # convert column Title of panda frame to list\n",
        "movie_synopses = doc_data['Synopsis'].tolist()\n",
        "movie_genre = doc_data['Genre'].tolist()\n",
        "doc_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk4y5Oag7h2i",
        "colab_type": "text"
      },
      "source": [
        "#Step 1:  Pre-Processing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llagIx34TcwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function for Lemmatization, remove stopword and feature selection using POS, spacy package\n",
        "import spacy\n",
        "def spacy_preprocess (text,lemma= True, pos= True, pos_select = [\"VERB\", \"NOUN\", \"ADJ\",\"ADV\",\"PART\"]):\n",
        "  # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
        "  nlp = spacy.load('en', disable=['parser', 'ner']) # disable parser, ner for faster loading\n",
        "  # Parse the sentence using the loaded 'en' model object `nlp`\n",
        "  doc = nlp(text)\n",
        "    \n",
        "  if pos== False:\n",
        "    if lemma== True: text_preprocess= \" \".join([token.lemma_.lower() for token in doc if not nlp.vocab[token.text].is_stop])\n",
        "    if lemma== False:text_preprocess= \" \".join([token.text.lower() for token in doc if not nlp.vocab[token.text].is_stop])\n",
        "  else:\n",
        "    if lemma== True : text_preprocess= \" \".join([token.lemma_.lower() for token in doc if (token.pos_ in pos_select and not nlp.vocab[token.text].is_stop)])\n",
        "    if lemma== False : text_preprocess= \" \".join([token.text.lower() for token in doc if (token.pos_ in pos_select  and not nlp.vocab[token.text].is_stop)])\n",
        "  # nlp.vocab[token.text].is_stop to remove stopwords\n",
        "  return text_preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vjU8QEaVMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing data with spacy\n",
        "from tqdm import tqdm\n",
        "movie_synopses_preprocess=[]\n",
        "for movie in tqdm(movie_synopses):\n",
        "  movie_preprocess = spacy_preprocess(movie,pos_select = [\"VERB\", \"NOUN\", \"ADJ\"])\n",
        "  movie_synopses_preprocess+= [movie_preprocess]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnsBqGk7VzE",
        "colab_type": "text"
      },
      "source": [
        "#Step 2: Build feature matrix (tf-idf)\n",
        "** you need to change the data input below** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_NBEusuYX7i",
        "colab_type": "text"
      },
      "source": [
        "##### data without preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHFumGWW5lD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#change data input (movie_synopses or movie_synopses_preprocess) here\n",
        "data_input =movie_synopses # data without preprocess\n",
        "#data_input =movie_synopses_preprocess  # the data after preprocess text.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.8, max_features= None)\n",
        "feature_matrix = vectorizer.fit_transform(data_input).astype(float)\n",
        "feature_names = vectorizer.get_feature_names() # get feature names\n",
        "print(\"number of feature:\", len(feature_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LER8RWg46yWZ",
        "colab_type": "text"
      },
      "source": [
        "#Step 3-1: Kmean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE0hJJtk9OcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "#function for Kmean clustering\n",
        "def k_means(feature_matrix, num_clusters=5):\n",
        "    km = KMeans(n_clusters=num_clusters, n_init=500, random_state = 1,\n",
        "                max_iter=10000)\n",
        "    km.fit(feature_matrix)\n",
        "    clusters = km.labels_\n",
        "    return km, clusters\n",
        "\n",
        "def get_cluster_data(clustering_obj, doc_data, \n",
        "                     feature_names, num_clusters,\n",
        "                     topn_features=10):\n",
        "\n",
        "    cluster_details = {}  \n",
        "    # get cluster centroids\n",
        "    ordered_centroids = clustering_obj.cluster_centers_.argsort()[:, ::-1]\n",
        "    # get key features for each cluster\n",
        "    # get docs belonging to each cluster\n",
        "    for cluster_num in range(num_clusters):\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster_num'] = cluster_num\n",
        "        key_features = [feature_names[index] \n",
        "                        for index \n",
        "                        in ordered_centroids[cluster_num, :topn_features]]\n",
        "        cluster_details[cluster_num]['key_features'] = key_features\n",
        "        \n",
        "        docs = doc_data[doc_data['Cluster'] == cluster_num]['Title'].values.tolist()\n",
        "        cluster_details[cluster_num]['docs'] = docs\n",
        "    \n",
        "    return cluster_details\n",
        "\n",
        "def print_clusters(cluster_data):\n",
        "    # print cluster details\n",
        "    for cluster_num, cluster_details in cluster_data.items():\n",
        "        print ('Cluster {} details:'.format(cluster_num))\n",
        "        print ('-'*20)\n",
        "        print ('Key features:', cluster_details['key_features'])\n",
        "        print ('docs in this cluster:')\n",
        "        print (', '.join(cluster_details['docs']))\n",
        "        print ('='*40)\n",
        "\n",
        "import time\n",
        "start=time.time()\n",
        "# assume that we want to clustering withy k =3\n",
        "num_clusters = 3\n",
        "km_obj, km_clusters = k_means(feature_matrix=feature_matrix,\n",
        "                           num_clusters=num_clusters)\n",
        "\n",
        "doc_data['Cluster'] = km_clusters\n",
        "\n",
        "km_cluster_data =  get_cluster_data(clustering_obj=km_obj,\n",
        "                                 doc_data = doc_data,\n",
        "                                 feature_names=feature_names,\n",
        "                                 num_clusters=num_clusters,\n",
        "                                 topn_features=10)\n",
        "\n",
        "print_clusters(km_cluster_data)  \n",
        "end=time.time()\n",
        "print(\"used time: \",end-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAZcHHNN638L",
        "colab_type": "text"
      },
      "source": [
        "#Step 3-2 Ward Hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1qihlkR5qNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to form and plot hierarchical clustering\n",
        "\n",
        "def ward_hierarchical_clustering(feature_matrix):\n",
        "    \n",
        "    cosine_distance = 1 - cosine_similarity(feature_matrix)\n",
        "    linkage_matrix = ward(cosine_distance)\n",
        "    return linkage_matrix\n",
        "\n",
        "def plot_hierarchical_clusters(linkage_matrix, doc_data, figure_size=(8,12)):\n",
        "    # set size\n",
        "    fig, ax = plt.subplots(figsize=figure_size,dpi=150) \n",
        "    doc_titles = doc_data['Title'].values.tolist()\n",
        "    # plot dendrogram\n",
        "    ax = dendrogram(linkage_matrix, orientation=\"left\", labels=doc_titles)\n",
        "    plt.tick_params(axis= 'x',   \n",
        "                    which='both',  \n",
        "                    bottom=False,\n",
        "                    top=False,\n",
        "                    labelbottom= False)\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig(mydrive+'hierachical_full.png', dpi=600)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "\n",
        "# build ward's linkage matrix    \n",
        "import time\n",
        "start = time.time()\n",
        "linkage_matrix = ward_hierarchical_clustering(feature_matrix)\n",
        "plot_hierarchical_clusters(linkage_matrix=linkage_matrix, doc_data=doc_data, figure_size=(6,8))\n",
        "end=time.time()\n",
        "print(\"used time: \",end-start)\n",
        "# you can find the file \"hierachical_full.png\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LENU3kRrg5T7",
        "colab_type": "text"
      },
      "source": [
        "#Step 3-3 SOM Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq1aQYTFgyWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install minisom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWyKjCzcYouu",
        "colab_type": "text"
      },
      "source": [
        "##### 8*8 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXtHYAwAgxun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 1 tranining the Neural network \n",
        "D = feature_matrix.todense().tolist()\n",
        "from minisom import MiniSom\n",
        "max_features=len(feature_names)\n",
        "xmap_dim =8\n",
        "ymap_dim =8\n",
        "\n",
        "\n",
        "som = MiniSom(x=xmap_dim, y= ymap_dim, input_len= max_features, random_seed=1)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#start training\n",
        "som.pca_weights_init(D)\n",
        "som.train_batch(data=D, num_iteration= max_features*200)\n",
        "#end tranining\n",
        "end = time.time()\n",
        "print(end - start,'seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LlKlewdIYwBq",
        "colab": {}
      },
      "source": [
        "# show cluster in movies titles\n",
        "titles=movie_titles\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, (t, vec) in enumerate(zip(titles, D)):\n",
        "    winnin_position = som.winner(vec)\n",
        "    plt.text(winnin_position[0], \n",
        "             winnin_position[1]+np.random.rand()*0.9, \n",
        "             t[0:20], color='black')\n",
        "    \n",
        "plt.xticks(range(xmap_dim))\n",
        "plt.yticks(range(ymap_dim))\n",
        "plt.grid()\n",
        "plt.xlim([0, xmap_dim])\n",
        "plt.ylim([0, ymap_dim])\n",
        "plt.plot()\n",
        "plt.savefig(mydrive+'som_titles.png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "supeUjbaYwB3",
        "colab": {}
      },
      "source": [
        "#Movies name, key feature and genre of every cluster\n",
        "top_keywords =10\n",
        "weights = som.get_weights()\n",
        "for i in range(xmap_dim):\n",
        "   for j in range(ymap_dim):\n",
        "      keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
        "      keywords = ' '.join([feature_names[k] for k in keywords_idx])\n",
        "      print('\\n')\n",
        "      print('Cell', i,'-',j, '-','keyword:', keywords)\n",
        "      movies_t =[]     \n",
        "      for k, (t, g, vec) in enumerate(zip(movie_titles, movie_genre, D)):\n",
        "        winnin_position = som.winner(vec)\n",
        "        if winnin_position[0]==i and winnin_position[1]==j: \n",
        "          movies_t+=[t]\n",
        "          \n",
        "      print('Titles:', movies_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjYW3u0_YzJD",
        "colab_type": "text"
      },
      "source": [
        "##### 10*10 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90KMR-kb25fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 1 tranining the Neural network \n",
        "D = feature_matrix.todense().tolist()\n",
        "from minisom import MiniSom\n",
        "max_features=len(feature_names)\n",
        "xmap_dim =10\n",
        "ymap_dim =10\n",
        "\n",
        "\n",
        "som = MiniSom(x=xmap_dim, y= ymap_dim, input_len= max_features, random_seed=1)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#start training\n",
        "som.pca_weights_init(D)\n",
        "som.train_batch(data=D, num_iteration= max_features*200)\n",
        "#end tranining\n",
        "end = time.time()\n",
        "print(end - start,'seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tjijN6Q-GEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show cluster in movies titles\n",
        "titles=movie_titles\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, (t, vec) in enumerate(zip(titles, D)):\n",
        "    winnin_position = som.winner(vec)\n",
        "    plt.text(winnin_position[0], \n",
        "             winnin_position[1]+np.random.rand()*0.9, \n",
        "             t[0:20], color='black')\n",
        "    \n",
        "plt.xticks(range(xmap_dim))\n",
        "plt.yticks(range(ymap_dim))\n",
        "plt.grid()\n",
        "plt.xlim([0, xmap_dim])\n",
        "plt.ylim([0, ymap_dim])\n",
        "plt.plot()\n",
        "plt.savefig(mydrive+'som_titles.png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y9Djlv4vo7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Movies name, key feature and genre of every cluster\n",
        "top_keywords =10\n",
        "weights = som.get_weights()\n",
        "for i in range(xmap_dim):\n",
        "   for j in range(ymap_dim):\n",
        "      keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
        "      keywords = ' '.join([feature_names[k] for k in keywords_idx])\n",
        "      print('\\n')\n",
        "      print('Cell', i,'-',j, '-','keyword:', keywords)\n",
        "      movies_t =[]     \n",
        "      for k, (t, g, vec) in enumerate(zip(movie_titles, movie_genre, D)):\n",
        "        winnin_position = som.winner(vec)\n",
        "        if winnin_position[0]==i and winnin_position[1]==j: \n",
        "          movies_t+=[t]\n",
        "          \n",
        "      print('Titles:', movies_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-LBFsbyY8TG",
        "colab_type": "text"
      },
      "source": [
        "##### data without preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AhFLlwjLhe_o"
      },
      "source": [
        "#Step 3-1: Kmean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1GhxcT22he_k",
        "colab": {}
      },
      "source": [
        "#change data input (movie_synopses or movie_synopses_preprocess) here\n",
        "# data_input =movie_synopses # data without preprocess\n",
        "data_input =movie_synopses_preprocess  # the data after preprocess text.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.8, max_features= None)\n",
        "# tfidf_vect=TfidfVectorizer(stop_words='english', max_features=300)\n",
        "feature_matrix = vectorizer.fit_transform(data_input).astype(float)\n",
        "feature_names = vectorizer.get_feature_names() # get feature names\n",
        "print(\"number of feature:\", len(feature_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pZXCIty8he_p",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "#function for Kmean clustering\n",
        "def k_means(feature_matrix, num_clusters=5):\n",
        "    km = KMeans(n_clusters=num_clusters, n_init=500, random_state = 1,\n",
        "                max_iter=10000)\n",
        "    km.fit(feature_matrix)\n",
        "    clusters = km.labels_\n",
        "    return km, clusters\n",
        "\n",
        "def get_cluster_data(clustering_obj, doc_data, \n",
        "                     feature_names, num_clusters,\n",
        "                     topn_features=10):\n",
        "\n",
        "    cluster_details = {}  \n",
        "    # get cluster centroids\n",
        "    ordered_centroids = clustering_obj.cluster_centers_.argsort()[:, ::-1]\n",
        "    # get key features for each cluster\n",
        "    # get docs belonging to each cluster\n",
        "    for cluster_num in range(num_clusters):\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster_num'] = cluster_num\n",
        "        key_features = [feature_names[index] \n",
        "                        for index \n",
        "                        in ordered_centroids[cluster_num, :topn_features]]\n",
        "        cluster_details[cluster_num]['key_features'] = key_features\n",
        "        \n",
        "        docs = doc_data[doc_data['Cluster'] == cluster_num]['Title'].values.tolist()\n",
        "        cluster_details[cluster_num]['docs'] = docs\n",
        "    \n",
        "    return cluster_details\n",
        "\n",
        "def print_clusters(cluster_data):\n",
        "    # print cluster details\n",
        "    for cluster_num, cluster_details in cluster_data.items():\n",
        "        print ('Cluster {} details:'.format(cluster_num))\n",
        "        print ('-'*20)\n",
        "        print ('Key features:', cluster_details['key_features'])\n",
        "        print ('docs in this cluster:')\n",
        "        print (', '.join(cluster_details['docs']))\n",
        "        print ('='*40)\n",
        "\n",
        "import time\n",
        "start=time.time()\n",
        "# assume that we want to clustering withy k =3\n",
        "num_clusters = 3\n",
        "km_obj, km_clusters = k_means(feature_matrix=feature_matrix,\n",
        "                           num_clusters=num_clusters)\n",
        "\n",
        "doc_data['Cluster'] = km_clusters\n",
        "\n",
        "km_cluster_data =  get_cluster_data(clustering_obj=km_obj,\n",
        "                                 doc_data = doc_data,\n",
        "                                 feature_names=feature_names,\n",
        "                                 num_clusters=num_clusters,\n",
        "                                 topn_features=10)\n",
        "\n",
        "print_clusters(km_cluster_data)  \n",
        "end=time.time()\n",
        "print(\"used time: \", end-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tUWvAk3bAlyR"
      },
      "source": [
        "#Step 3-2 Ward Hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7sl5uhIqAlyT",
        "colab": {}
      },
      "source": [
        "# Function to form and plot hierarchical clustering\n",
        "\n",
        "def ward_hierarchical_clustering(feature_matrix):\n",
        "    \n",
        "    cosine_distance = 1 - cosine_similarity(feature_matrix)\n",
        "    linkage_matrix = ward(cosine_distance)\n",
        "    return linkage_matrix\n",
        "\n",
        "def plot_hierarchical_clusters(linkage_matrix, doc_data, figure_size=(8,12)):\n",
        "    # set size\n",
        "    fig, ax = plt.subplots(figsize=figure_size,dpi=150) \n",
        "    doc_titles = doc_data['Title'].values.tolist()\n",
        "    # plot dendrogram\n",
        "    ax = dendrogram(linkage_matrix, orientation=\"left\", labels=doc_titles)\n",
        "    plt.tick_params(axis= 'x',   \n",
        "                    which='both',  \n",
        "                    bottom=False,\n",
        "                    top=False,\n",
        "                    labelbottom= False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(mydrive+'hierachical_full.png', dpi=600)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "\n",
        "# build ward's linkage matrix  \n",
        "import time\n",
        "start=time.time() \n",
        "linkage_matrix = ward_hierarchical_clustering(feature_matrix)\n",
        "plot_hierarchical_clusters(linkage_matrix=linkage_matrix, doc_data=doc_data, figure_size=(6,8))\n",
        "end=time.time()\n",
        "print(\"used time: \", end-start)\n",
        "# you can find the file \"hierachical_full.png\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fKhEEpRcEl1-"
      },
      "source": [
        "#Step 3-3 SOM Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrr_pjnZAlyZ",
        "colab": {}
      },
      "source": [
        "!pip install minisom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cURrou4VZI_n",
        "colab_type": "text"
      },
      "source": [
        "##### 8*8 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5dGKgNqAlyc",
        "colab": {}
      },
      "source": [
        "#Step 1 tranining the Neural network \n",
        "D = feature_matrix.todense().tolist()\n",
        "from minisom import MiniSom\n",
        "max_features=len(feature_names)\n",
        "xmap_dim =8\n",
        "ymap_dim =8\n",
        "# 8*8 neuron\n",
        "\n",
        "som = MiniSom(x=xmap_dim, y= ymap_dim, input_len= max_features, random_seed=1)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#start training\n",
        "som.pca_weights_init(D)\n",
        "som.train_batch(data=D, num_iteration= max_features*200)\n",
        "#end tranining\n",
        "end = time.time()\n",
        "print(end - start,'seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q92hgWZGZG-4",
        "colab": {}
      },
      "source": [
        "# show cluster in movies titles\n",
        "titles=movie_titles\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, (t, vec) in enumerate(zip(titles, D)):\n",
        "    winnin_position = som.winner(vec)\n",
        "    plt.text(winnin_position[0], \n",
        "             winnin_position[1]+np.random.rand()*0.9, \n",
        "             t[0:20], color='black')\n",
        "    \n",
        "  \n",
        "plt.xticks(range(xmap_dim))\n",
        "plt.yticks(range(ymap_dim))\n",
        "plt.grid()\n",
        "plt.xlim([0, xmap_dim])\n",
        "plt.ylim([0, ymap_dim])\n",
        "plt.plot()\n",
        "plt.savefig(mydrive+'som_titles.png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J0cyu3xgZG_E",
        "colab": {}
      },
      "source": [
        "#Movies name, key feature and genre of every cluster\n",
        "top_keywords =10\n",
        "weights = som.get_weights()\n",
        "for i in range(xmap_dim):\n",
        "   for j in range(ymap_dim):\n",
        "      keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
        "      keywords = ' '.join([feature_names[k] for k in keywords_idx])\n",
        "      print('\\n')\n",
        "      print('Cell', i,'-',j, '-','keyword:', keywords)\n",
        "      movies_t =[]     \n",
        "      for k, (t, g, vec) in enumerate(zip(movie_titles, movie_genre, D)):\n",
        "        winnin_position = som.winner(vec)\n",
        "        if winnin_position[0]==i and winnin_position[1]==j: \n",
        "          movies_t+=[t]\n",
        "          \n",
        "      print('Titles:', movies_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6uqpdG9ZM4u",
        "colab_type": "text"
      },
      "source": [
        "##### 10*10 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MDuIJI5BfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 1 tranining the Neural network \n",
        "D = feature_matrix.todense().tolist()\n",
        "from minisom import MiniSom\n",
        "max_features=len(feature_names)\n",
        "xmap_dim =10\n",
        "ymap_dim =10\n",
        "# 10*10 neuron\n",
        "\n",
        "som = MiniSom(x=xmap_dim, y= ymap_dim, input_len= max_features, random_seed=1)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#start training\n",
        "som.pca_weights_init(D)\n",
        "som.train_batch(data=D, num_iteration= max_features*200)\n",
        "#end tranining\n",
        "end = time.time()\n",
        "print(end - start,'seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sbgdpjygAlyg",
        "colab": {}
      },
      "source": [
        "# show cluster in movies titles\n",
        "titles=movie_titles\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, (t, vec) in enumerate(zip(titles, D)):\n",
        "    winnin_position = som.winner(vec)\n",
        "    plt.text(winnin_position[0], \n",
        "             winnin_position[1]+np.random.rand()*0.9, \n",
        "             t[0:20], color='black')\n",
        "    \n",
        "  \n",
        "plt.xticks(range(xmap_dim))\n",
        "plt.yticks(range(ymap_dim))\n",
        "plt.grid()\n",
        "plt.xlim([0, xmap_dim])\n",
        "plt.ylim([0, ymap_dim])\n",
        "plt.plot()\n",
        "plt.savefig(mydrive+'som_titles.png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aj8ys7iFAlyk",
        "colab": {}
      },
      "source": [
        "#Movies name, key feature and genre of every cluster\n",
        "top_keywords =10\n",
        "weights = som.get_weights()\n",
        "for i in range(xmap_dim):\n",
        "   for j in range(ymap_dim):\n",
        "      keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
        "      keywords = ' '.join([feature_names[k] for k in keywords_idx])\n",
        "      print('\\n')\n",
        "      print('Cell', i,'-',j, '-','keyword:', keywords)\n",
        "      movies_t =[]     \n",
        "      for k, (t, g, vec) in enumerate(zip(movie_titles, movie_genre, D)):\n",
        "        winnin_position = som.winner(vec)\n",
        "        if winnin_position[0]==i and winnin_position[1]==j: \n",
        "          movies_t+=[t]\n",
        "          \n",
        "      print('Titles:', movies_t)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}