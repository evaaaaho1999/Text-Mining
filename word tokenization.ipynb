{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Read text from file carroll-alice.txt”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\eva\\Downloads\\carroll-alice.txt\",\"r\") as f:\n",
    "    data=f.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Normalize words: convert all upper-case letter to lower case (for example 'Word' and'word' are considered as the same word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data_removed_pun=re.sub(r'[^\\w]',' ',data) \n",
    "print(data_removed_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_split = data_removed_pun.lower().split()\n",
    "print (tokens_split )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)Tokenization: using nltk word_tokenize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "token=nltk.word_tokenize(data)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Remove stopwords using stopwords from nltk. You can add more stopword if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "filter_token=[word for word in token if not word in stopwords]\n",
    "user_stopwords=[\"CHAPTER\",\"chapter\",\"s\",\"[\",\"]\",\"(\",\")\",\"!\",\"?\"]\n",
    "filter_token_2=[word for word in filter_token if not word in user_stopwords]\n",
    "filter_token_3=[word for word in tokens_split if not word in stopwords and not word in user_stopwords]\n",
    "#print(token)\n",
    "#print(filter_token)\n",
    "#print(filter_token_2)\n",
    "print(filter_token_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) Count the occurrence of words and display the top 10 common words. (the result will show the most common word and the counting value of this word in the document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term,token_doc):\n",
    "    tf=token_doc.count(term)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf={}\n",
    "for word in filter_token_3:\n",
    "    data_tf[word]=tf(word,filter_token_3)\n",
    "# print(data_tf.items())\n",
    "L=sorted(data_tf.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Top 10 frequency words are:\")\n",
    "Token=[]\n",
    "for i in range(10):\n",
    "    Token.append(L[i])\n",
    "    print(L[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (*) Use Porter or Lancaster stemming and Lemmatization function from nltk package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "token_psstem = [ps.stem(word) for word in filter_token_3]\n",
    "print(token_psstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import LancasterStemmer\n",
    "# ls = LancasterStemmer()\n",
    "# token_lsstem = [ls.stem(word) for word in tokens_split]\n",
    "# print(token_lsstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psstem_count={}\n",
    "for word in token_psstem:\n",
    "    psstem_count[word]=tf(word,token_psstem)\n",
    "#print(psstem_count.items())\n",
    "PS=sorted(psstem_count.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Top 10 frequency words by Lancaster stemming are:\")\n",
    "Stem=[]\n",
    "for i in range(10):\n",
    "    Stem.append(PS[i])\n",
    "    print(PS[i])\n",
    "# lsstem_count={}\n",
    "# for word in token_lsstem:\n",
    "#     lsstem_count[word]=tf(word,token_lsstem)\n",
    "# #print(lsstem_count.items())\n",
    "# LS=sorted(lsstem_count.items(), key=lambda x:x[1], reverse=True)\n",
    "# for i in range(10):\n",
    "#     print(LS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (*)Optional: you can use Lemmatization using the spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk import word_tokenize, pos_tag\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# def is_noun(tag):\n",
    "#     return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "# def is_verb(tag):\n",
    "#     return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "# def is_adverb(tag):\n",
    "#     return tag in ['RB', 'RBR', 'RBS']\n",
    "# def is_adjective(tag):\n",
    "#     return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# def get_wordnet_pos(tag):\n",
    "#     if is_adjective(tag):\n",
    "#         return wordnet.ADJ\n",
    "#     elif is_noun(tag):\n",
    "#         return wordnet.NOUN\n",
    "#     elif is_adverb(tag):\n",
    "#         return wordnet.ADV\n",
    "#     elif is_verb(tag):\n",
    "#         return wordnet.VERB\n",
    "#     return None\n",
    "# def get_wordnet_pos(tag):\n",
    "#     if tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return None\n",
    "# wnl = WordNetLemmatizer()\n",
    "# tagged_sent = pos_tag(tokens_split)\n",
    "# lemmas_sent = []\n",
    "# for tag in tagged_sent:\n",
    "#     wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "#     lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 詞形還原\n",
    "\n",
    "# print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python -m spacy validate\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-1.2.0/en_core_web_md-1.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(data_removed_pun.lower())\n",
    "temp=[]\n",
    "for token in doc:\n",
    "    if token.lemma_ == '-PRON-':\n",
    "        token.lemma_ = token.orth_ \n",
    "    else: \n",
    "        temp.append(token.lemma_)\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "clean=[' ','s','  ','   ','    ']\n",
    "for item in temp:\n",
    "    if item not in clean:\n",
    "        if item not in stopwords:\n",
    "            result.append(item)                                      \n",
    "# print (result)\n",
    "result_tf={}\n",
    "for word in result:\n",
    "    result_tf[word]=tf(word,result)\n",
    "# print(data_tf.items())\n",
    "R=sorted(result_tf.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Top 10 frequency words by lemmitization are:\")\n",
    "Lemma=[]\n",
    "for i in range(10):\n",
    "    Lemma.append(R[i])\n",
    "    print(R[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (*)Top 10 frequency words outcome by using different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequency words are:\n",
      " [('said', 462), ('alice', 398), ('little', 128), ('one', 104), ('know', 88), ('like', 85), ('would', 83), ('went', 83), ('could', 77), ('queen', 75)]\n",
      "Top 10 frequency words by Lancaster stemming are:\n",
      " [('said', 462), ('alic', 398), ('littl', 128), ('look', 106), ('one', 105), ('like', 97), ('know', 92), ('would', 83), ('went', 83), ('thought', 80)]\n",
      "Top 10 frequency words by lemmitization are:\n",
      " [('say', 532), ('alice', 398), ('go', 180), ('think', 133), ('little', 128), ('get', 113), ('know', 107), ('look', 106), ('one', 105), ('see', 97)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 frequency words are:\\n\",Token)\n",
    "print(\"Top 10 frequency words by Lancaster stemming are:\\n\",Stem)\n",
    "print(\"Top 10 frequency words by lemmitization are:\\n\",Lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_dict = {\"tokenization\":data_tf,\"stemming\":psstem_count,\"Lemmatization\":result_tf}\n",
    "tf_dataframe = pd.DataFrame(tf_dict).transpose()\n",
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results are different in the three methods. By tokenization, the words are still have different tenses and types, eg. 'could', so it just count which words appear in the text most often. While for the stemming, it 'reduce' the words, eg. 'little' becomes 'littl'; therefore, sometimes this method may not provide a maeningful and comeplete outcome, while it can be used in searching realm. For the lemmatization, it 'transform' the words, eg. 'said' becomes 'say', that is, it can find the same word in different tense and types, but it will be more complicate to do. In my opinion, I think the lemmatiation is the better way to analyze the text for it provide a more precise, correct and complete outcome, besides, the outcome is meaningful and can be used to do research or analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
