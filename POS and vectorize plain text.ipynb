{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\eva\\Desktop\\作業 上課資料(清大)\\大三上\\文件探勘\\HW\\HW04\\assignment data\\doc1.txt\",\"r\") as f:\n",
    "    data1=f.read()\n",
    "with open(r\"C:\\Users\\eva\\Desktop\\作業 上課資料(清大)\\大三上\\文件探勘\\HW\\HW04\\assignment data\\doc2.txt\",\"r\") as f:\n",
    "    data2=f.read()\n",
    "with open(r\"C:\\Users\\eva\\Desktop\\作業 上課資料(清大)\\大三上\\文件探勘\\HW\\HW04\\assignment data\\doc3.txt\",\"r\") as f:\n",
    "    data3=f.read()\n",
    "#print(data1,data2,data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[('get', 'NN'), ('started', 'VBD'), ('python', 'JJ'), ('sections', 'NNS'), ('linked', 'VBN'), ('left', 'JJ'), ('python', 'NN'), ('set', 'VBN'), ('get', 'VB'), ('python', 'JJ'), ('installed', 'VBN'), ('machine', 'NN'), ('python', 'NN'), ('introduction', 'NN'), ('introduction', 'NN'), ('language', 'NN'), ('python', 'NN'), ('strings', 'NNS'), ('starts', 'VBZ'), ('coding', 'VBG'), ('material', 'NN'), ('leading', 'VBG'), ('first', 'JJ'), ('exercise', 'JJ'), ('end', 'NN'), ('written', 'VBN'), ('section', 'NN'), ('includes', 'VBZ'), ('link', 'VBP'), ('code', 'NN'), ('exercise', 'NN'), ('section', 'NN'), ('material', 'NN'), ('lecture', 'NN'), ('videos', 'FW'), ('parallel', 'JJ'), ('written', 'VBN'), ('materials', 'NNS'), ('introducing', 'VBG'), ('python', 'NN'), ('strings', 'NNS'), ('first', 'JJ'), ('exercises', 'VBZ'), ('google', 'JJ'), ('material', 'NN'), ('makes', 'VBZ'), ('intensive', 'JJ'), ('2', 'CD'), ('day', 'NN'), ('class', 'NN'), ('videos', 'NNS'), ('organized', 'VBD'), ('day', 'NN'), ('1', 'CD'), ('day', 'NN'), ('2', 'CD'), ('sections', 'NNS'), ('material', 'NN'), ('created', 'VBD'), ('nick', 'JJ'), ('parlante', 'NN'), ('working', 'VBG'), ('engedu', 'JJ'), ('group', 'NN'), ('google', 'VBD'), ('special', 'JJ'), ('thanks', 'NNS'), ('help', 'VBP'), ('google', 'VB'), ('colleagues', 'NNS'), ('john', 'VB'), ('cox', 'NNS'), ('steve', 'VBP'), ('glassman', 'NN'), ('piotr', 'NN'), ('kaminksi', 'FW'), ('antoine', 'JJ'), ('picard', 'NN'), ('finally', 'RB'), ('thanks', 'NNS'), ('google', 'VBP'), ('director', 'NN'), ('maggie', 'NN'), ('johnson', 'NN'), ('enlightened', 'VBD'), ('generosity', 'NN'), ('put', 'VBN'), ('materials', 'NNS'), ('internet', 'VBP'), ('free', 'JJ'), ('creative', 'JJ'), ('commons', 'NNS'), ('attribution', 'VBP'), ('2', 'CD'), ('5', 'CD'), ('license', 'NN'), ('share', 'NN'), ('enjoy', 'NN')]\n",
      "[('welcome', 'JJ'), ('google', 'NN'), ('python', 'NN'), ('online', 'VBP'), ('tutorial', 'NN'), ('based', 'VBN'), ('introductory', 'JJ'), ('python', 'JJ'), ('course', 'NN'), ('offered', 'VBN'), ('internally', 'RB'), ('originally', 'RB'), ('created', 'VBN'), ('python', 'NN'), ('2', 'CD'), ('4', 'CD'), ('days', 'NNS'), ('tried', 'VBD'), ('keep', 'JJ'), ('content', 'NN'), ('universal', 'NN'), ('exercises', 'VBZ'), ('relevant', 'JJ'), ('even', 'RB'), ('newer', 'JJR'), ('releases', 'NNS'), ('mentioned', 'VBD'), ('setup', 'JJ'), ('page', 'NN'), ('material', 'NN'), ('covers', 'VBZ'), ('python', 'JJ'), ('recommend', 'NN'), ('avoiding', 'VBG'), ('python', 'JJ'), ('recognize', 'VB'), ('future', 'JJ'), ('new', 'JJ'), ('features', 'NNS'), ('going', 'VBG'), ('good', 'JJ'), ('news', 'NN'), ('developers', 'NNS'), ('learning', 'VBG'), ('either', 'DT'), ('version', 'NN'), ('pick', 'NN'), ('without', 'IN'), ('much', 'JJ'), ('difficulty', 'NN'), ('want', 'VBP'), ('know', 'JJ'), ('choosing', 'VBG'), ('python', 'JJ'), ('check', 'NN'), ('post', 'NN')]\n",
      "[('need', 'NN'), ('quick', 'JJ'), ('brush', 'NN'), ('learning', 'VBG'), ('python', 'IN'), ('first', 'JJ'), ('time', 'NN'), ('come', 'VB'), ('right', 'JJ'), ('place', 'NN'), ('let', 'VB'), ('get', 'VB'), ('started', 'VBN'), ('learning', 'VBG'), ('one', 'CD'), ('easiest', 'JJS'), ('coding', 'NN'), ('languages', 'NNS'), ('right', 'RB'), ('need', 'VBP'), ('fret', 'RB'), ('coded', 'JJ'), ('time', 'NN'), ('finish', 'JJ'), ('course', 'NN'), ('pro', 'JJ'), ('python', 'NN'), ('python', 'NN'), ('great', 'JJ'), ('friendly', 'JJ'), ('language', 'NN'), ('use', 'NN'), ('learn', 'FW'), ('fun', 'NN'), ('adapted', 'VBD'), ('small', 'JJ'), ('large', 'JJ'), ('projects', 'NNS'), ('python', 'VBP'), ('cut', 'VB'), ('development', 'NN'), ('time', 'NN'), ('greatly', 'RB'), ('overall', 'JJ'), ('much', 'JJ'), ('faster', 'JJR'), ('write', 'JJ'), ('python', 'NN'), ('languages', 'NNS'), ('course', 'NN'), ('quick', 'JJ'), ('way', 'NN'), ('understand', 'IN'), ('major', 'JJ'), ('concepts', 'NNS'), ('python', 'VBP'), ('programming', 'VBG'), ('whiz', 'JJ'), ('time', 'NN'), ('course', 'NN'), ('one', 'CD'), ('stop', 'NN'), ('shop', 'NN'), ('everything', 'NN'), ('need', 'NN'), ('know', 'VBP'), ('get', 'VB'), ('started', 'VBN'), ('python', 'RB'), ('along', 'IN'), ('incentives', 'NNS'), ('begin', 'VBP'), ('basics', 'NNS'), ('python', 'VBP'), ('learning', 'VBG'), ('strings', 'NNS'), ('variables', 'NNS'), ('getting', 'VBG'), ('know', 'VBP'), ('data', 'NNS'), ('types', 'NNS'), ('soon', 'RB'), ('move', 'VBP'), ('loops', 'JJ'), ('conditions', 'NNS'), ('python', 'VBP'), ('afterwards', 'NNS'), ('discuss', 'JJ'), ('bit', 'NN'), ('file', 'JJ'), ('manipulation', 'NN'), ('functions', 'NNS'), ('know', 'VBP'), ('basics', 'NNS'), ('python', 'VBP'), ('hope', 'NN'), ('excited', 'VBN'), ('dive', 'JJ'), ('world', 'NN'), ('python', 'NN'), ('course', 'NN'), ('well', 'RB'), ('waiting', 'VBG'), ('let', 'VB'), ('get', 'VB'), ('started', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "data1_1=re.sub(r'[^\\w]',' ',data1) \n",
    "data2_1=re.sub(r'[^\\w]',' ',data2) \n",
    "data3_1=re.sub(r'[^\\w]',' ',data3) \n",
    "#print(data1_1)\n",
    "tokens1 = data1_1.lower()\n",
    "tokens2 = data2_1.lower()\n",
    "tokens3 = data3_1.lower()\n",
    "tokens1 = nltk.word_tokenize(tokens1)\n",
    "tokens2 = nltk.word_tokenize(tokens2)\n",
    "tokens3 = nltk.word_tokenize(tokens3)\n",
    "\n",
    "\n",
    "# token=nltk.word_tokenize(tokens_all.values())\n",
    "# print(token)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "filter_token1=[word for word in tokens1 if not word in stopwords]\n",
    "filter_token2=[word for word in tokens2 if not word in stopwords]\n",
    "filter_token3=[word for word in tokens3 if not word in stopwords]\n",
    "\n",
    "tokens_all={\"doc1\":filter_token1, \"doc2\":filter_token2, \"doc3\":filter_token3}\n",
    "# print (tokens_all)\n",
    "\n",
    "for word in tokens_all.keys():\n",
    "    print(nltk.pos_tag(tokens_all[word]))\n",
    "# print(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import spacy\n",
    "# def tf(term,token_doc):\n",
    "#     tf=token_doc.count(term)\n",
    "#     return tf\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "# from nltk.corpus import stopwords\n",
    "# doc_all=nlp(tokens_all)\n",
    "# # print(doc_all)\n",
    "# temp=[]\n",
    "# for token in doc_all:\n",
    "#     if token.lemma_ == '-PRON-':\n",
    "#         token.lemma_ = token.orth_ \n",
    "#     else: \n",
    "#         temp.append(token.lemma_)\n",
    "# #     print(token.lemma_)\n",
    "# result = []\n",
    "# clean=[' ','s','  ','   ','    ']\n",
    "# for item in temp:\n",
    "#     if item not in clean:\n",
    "#         if item not in stopwords.words('english'):\n",
    "#             result.append(item)                                      \n",
    "# print (result)\n",
    "# result_tf={}\n",
    "# for word in result:\n",
    "#     result_tf[word]=tf(word,result)\n",
    "# # print(data_tf.items())\n",
    "# R=sorted(result_tf.items(), key=lambda x:x[1], reverse=True)\n",
    "# print(\"Top 10 frequency words by lemmitization are:\")\n",
    "# Lemma=[]\n",
    "# for i in range(10):\n",
    "#     Lemma.append(R[i])\n",
    "#     print(R[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\eva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "{'doc1': ['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'end', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'day', 'class', 'videos', 'organized', 'day', 'day', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'antoine', 'picard', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy'], 'doc2': ['welcome', 'google', 'python', 'online', 'tutorial', 'based', 'introductory', 'python', 'course', 'offered', 'created', 'python', 'days', 'tried', 'keep', 'content', 'universal', 'exercises', 'relevant', 'newer', 'releases', 'mentioned', 'setup', 'page', 'material', 'covers', 'python', 'recommend', 'avoiding', 'python', 'recognize', 'future', 'new', 'features', 'going', 'good', 'news', 'developers', 'learning', 'version', 'pick', 'much', 'difficulty', 'want', 'know', 'choosing', 'python', 'check', 'post'], 'doc3': ['need', 'quick', 'brush', 'learning', 'first', 'time', 'come', 'right', 'place', 'let', 'get', 'started', 'learning', 'easiest', 'coding', 'languages', 'need', 'coded', 'time', 'finish', 'course', 'pro', 'python', 'python', 'great', 'friendly', 'language', 'use', 'fun', 'adapted', 'small', 'large', 'projects', 'python', 'cut', 'development', 'time', 'overall', 'much', 'faster', 'write', 'python', 'languages', 'course', 'quick', 'way', 'major', 'concepts', 'python', 'programming', 'whiz', 'time', 'course', 'stop', 'shop', 'everything', 'need', 'know', 'get', 'started', 'incentives', 'begin', 'basics', 'python', 'learning', 'strings', 'variables', 'getting', 'know', 'data', 'types', 'move', 'loops', 'conditions', 'python', 'afterwards', 'discuss', 'bit', 'file', 'manipulation', 'functions', 'know', 'basics', 'python', 'hope', 'excited', 'dive', 'world', 'python', 'course', 'waiting', 'let', 'get', 'started']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "tagged_sent1 = nltk.pos_tag(filter_token1)\n",
    "tagged_sent2 = nltk.pos_tag(filter_token2)\n",
    "tagged_sent3 = nltk.pos_tag(filter_token3)\n",
    "new_token1 = []\n",
    "new_token2 = []\n",
    "new_token3 = []\n",
    "pos_need=['NN', 'NNS', 'NNP', 'NNPS','VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ','JJ', 'JJR', 'JJS']\n",
    "for word,pos in tagged_sent1:\n",
    "    if (pos in pos_need):\n",
    "        new_token1.append(word)\n",
    "for word,pos in tagged_sent2:\n",
    "    if (pos in pos_need):\n",
    "        new_token2.append(word)\n",
    "for word,pos in tagged_sent3:\n",
    "    if (pos in pos_need):\n",
    "        new_token3.append(word)\n",
    "# print(new_token1,new_token2,new_token3)\n",
    "tokens_all_new={\"doc1\":new_token1, \"doc2\":new_token2, \"doc3\":new_token3}\n",
    "print(tokens_all_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf function\n",
    "def tf(term, token_doc):\n",
    "    tf = token_doc.count(term)/len(token_doc)\n",
    "    return tf\n",
    "\n",
    "# create function to calculate how many doc contain the term \n",
    "def numDocsContaining(word, token_doclist):\n",
    "    doccount = 0\n",
    "    for doc_token in token_doclist:\n",
    "        if doc_token.count(word) > 0:\n",
    "            doccount +=1\n",
    "    return doccount\n",
    "  \n",
    "import math\n",
    "# create function to calculate  Inverse Document Frequency in doclist\n",
    "def idf(word, token_doclist):\n",
    "    n = len(token_doclist)\n",
    "    df = numDocsContaining(word, token_doclist)\n",
    "    return math.log10(n/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': 0.47712125471966244, 'pro': 0.47712125471966244, 'materials': 0.47712125471966244, 'free': 0.47712125471966244, 'version': 0.47712125471966244, 'python': 0.0, 'pick': 0.47712125471966244, 'left': 0.47712125471966244, 'know': 0.17609125905568124, 'help': 0.47712125471966244, 'online': 0.47712125471966244, 'end': 0.47712125471966244, 'time': 0.47712125471966244, 'internet': 0.47712125471966244, 'exercise': 0.47712125471966244, 'choosing': 0.47712125471966244, 'future': 0.47712125471966244, 'class': 0.47712125471966244, 'difficulty': 0.47712125471966244, 'dive': 0.47712125471966244, 'put': 0.47712125471966244, 'come': 0.47712125471966244, 'created': 0.17609125905568124, 'going': 0.47712125471966244, 'code': 0.47712125471966244, 'introductory': 0.47712125471966244, 'coded': 0.47712125471966244, 'concepts': 0.47712125471966244, 'parlante': 0.47712125471966244, 'fun': 0.47712125471966244, 'course': 0.17609125905568124, 'variables': 0.47712125471966244, 'strings': 0.17609125905568124, 'write': 0.47712125471966244, 'content': 0.47712125471966244, 'stop': 0.47712125471966244, 'mentioned': 0.47712125471966244, 'johnson': 0.47712125471966244, 'developers': 0.47712125471966244, 'discuss': 0.47712125471966244, 'great': 0.47712125471966244, 'begin': 0.47712125471966244, 'recommend': 0.47712125471966244, 'newer': 0.47712125471966244, 'features': 0.47712125471966244, 'introduction': 0.47712125471966244, 'group': 0.47712125471966244, 'picard': 0.47712125471966244, 'offered': 0.47712125471966244, 'license': 0.47712125471966244, 'universal': 0.47712125471966244, 'languages': 0.47712125471966244, 'working': 0.47712125471966244, 'way': 0.47712125471966244, 'intensive': 0.47712125471966244, 'thanks': 0.47712125471966244, 'material': 0.17609125905568124, 'hope': 0.47712125471966244, 'tried': 0.47712125471966244, 'finish': 0.47712125471966244, 'everything': 0.47712125471966244, 'set': 0.47712125471966244, 'much': 0.17609125905568124, 'cut': 0.47712125471966244, 'share': 0.47712125471966244, 'adapted': 0.47712125471966244, 'projects': 0.47712125471966244, 'includes': 0.47712125471966244, 'shop': 0.47712125471966244, 'learning': 0.17609125905568124, 'new': 0.47712125471966244, 'creative': 0.47712125471966244, 'development': 0.47712125471966244, 'types': 0.47712125471966244, 'john': 0.47712125471966244, 'waiting': 0.47712125471966244, 'easiest': 0.47712125471966244, 'conditions': 0.47712125471966244, 'enjoy': 0.47712125471966244, 'incentives': 0.47712125471966244, 'afterwards': 0.47712125471966244, 'get': 0.17609125905568124, 'file': 0.47712125471966244, 'getting': 0.47712125471966244, 'starts': 0.47712125471966244, 'google': 0.17609125905568124, 'piotr': 0.47712125471966244, 'functions': 0.47712125471966244, 'based': 0.47712125471966244, 'basics': 0.47712125471966244, 'covers': 0.47712125471966244, 'avoiding': 0.47712125471966244, 'written': 0.47712125471966244, 'commons': 0.47712125471966244, 'post': 0.47712125471966244, 'parallel': 0.47712125471966244, 'generosity': 0.47712125471966244, 'bit': 0.47712125471966244, 'colleagues': 0.47712125471966244, 'whiz': 0.47712125471966244, 'right': 0.47712125471966244, 'programming': 0.47712125471966244, 'introducing': 0.47712125471966244, 'special': 0.47712125471966244, 'friendly': 0.47712125471966244, 'check': 0.47712125471966244, 'need': 0.47712125471966244, 'days': 0.47712125471966244, 'linked': 0.47712125471966244, 'coding': 0.17609125905568124, 'welcome': 0.47712125471966244, 'quick': 0.47712125471966244, 'data': 0.47712125471966244, 'brush': 0.47712125471966244, 'installed': 0.47712125471966244, 'excited': 0.47712125471966244, 'want': 0.47712125471966244, 'move': 0.47712125471966244, 'makes': 0.47712125471966244, 'enlightened': 0.47712125471966244, 'setup': 0.47712125471966244, 'started': 0.17609125905568124, 'leading': 0.47712125471966244, 'tutorial': 0.47712125471966244, 'link': 0.47712125471966244, 'engedu': 0.47712125471966244, 'first': 0.17609125905568124, 'lecture': 0.47712125471966244, 'glassman': 0.47712125471966244, 'place': 0.47712125471966244, 'recognize': 0.47712125471966244, 'faster': 0.47712125471966244, 'overall': 0.47712125471966244, 'steve': 0.47712125471966244, 'section': 0.47712125471966244, 'keep': 0.47712125471966244, 'machine': 0.47712125471966244, 'exercises': 0.17609125905568124, 'organized': 0.47712125471966244, 'major': 0.47712125471966244, 'world': 0.47712125471966244, 'news': 0.47712125471966244, 'attribution': 0.47712125471966244, 'use': 0.47712125471966244, 'cox': 0.47712125471966244, 'director': 0.47712125471966244, 'loops': 0.47712125471966244, 'small': 0.47712125471966244, 'sections': 0.47712125471966244, 'page': 0.47712125471966244, 'language': 0.17609125905568124, 'videos': 0.47712125471966244, 'antoine': 0.47712125471966244, 'large': 0.47712125471966244, 'relevant': 0.47712125471966244, 'good': 0.47712125471966244, 'releases': 0.47712125471966244, 'let': 0.47712125471966244, 'nick': 0.47712125471966244, 'maggie': 0.47712125471966244, 'manipulation': 0.47712125471966244}\n"
     ]
    }
   ],
   "source": [
    "# numDocsContaining('python', tokens_all_new.values())\n",
    "use_word=[]\n",
    "for word in tokens_all_new.keys():\n",
    "    use_word+= tokens_all_new[word]\n",
    "use_word=set(use_word)\n",
    "# print(use_word)\n",
    "# calculate idf for every word in bag_words\n",
    "use_words_idf={} # declare \"bag_words_idf\" data structure is dictionary \n",
    "for word in use_word:\n",
    "    use_words_idf[word]= idf(word,tokens_all_new.values())#對應到字典中該字的值\n",
    "print(use_words_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': {'antoine': 0.005301347274662916,\n",
       "  'attribution': 0.005301347274662916,\n",
       "  'class': 0.005301347274662916,\n",
       "  'code': 0.005301347274662916,\n",
       "  'coding': 0.001956569545063125,\n",
       "  'colleagues': 0.005301347274662916,\n",
       "  'commons': 0.005301347274662916,\n",
       "  'cox': 0.005301347274662916,\n",
       "  'created': 0.001956569545063125,\n",
       "  'creative': 0.005301347274662916,\n",
       "  'day': 0.015904041823988746,\n",
       "  'director': 0.005301347274662916,\n",
       "  'end': 0.005301347274662916,\n",
       "  'engedu': 0.005301347274662916,\n",
       "  'enjoy': 0.005301347274662916,\n",
       "  'enlightened': 0.005301347274662916,\n",
       "  'exercise': 0.010602694549325832,\n",
       "  'exercises': 0.001956569545063125,\n",
       "  'first': 0.00391313909012625,\n",
       "  'free': 0.005301347274662916,\n",
       "  'generosity': 0.005301347274662916,\n",
       "  'get': 0.00391313909012625,\n",
       "  'glassman': 0.005301347274662916,\n",
       "  'google': 0.0078262781802525,\n",
       "  'group': 0.005301347274662916,\n",
       "  'help': 0.005301347274662916,\n",
       "  'includes': 0.005301347274662916,\n",
       "  'installed': 0.005301347274662916,\n",
       "  'intensive': 0.005301347274662916,\n",
       "  'internet': 0.005301347274662916,\n",
       "  'introducing': 0.005301347274662916,\n",
       "  'introduction': 0.010602694549325832,\n",
       "  'john': 0.005301347274662916,\n",
       "  'johnson': 0.005301347274662916,\n",
       "  'language': 0.001956569545063125,\n",
       "  'leading': 0.005301347274662916,\n",
       "  'lecture': 0.005301347274662916,\n",
       "  'left': 0.005301347274662916,\n",
       "  'license': 0.005301347274662916,\n",
       "  'link': 0.005301347274662916,\n",
       "  'linked': 0.005301347274662916,\n",
       "  'machine': 0.005301347274662916,\n",
       "  'maggie': 0.005301347274662916,\n",
       "  'makes': 0.005301347274662916,\n",
       "  'material': 0.0078262781802525,\n",
       "  'materials': 0.010602694549325832,\n",
       "  'nick': 0.005301347274662916,\n",
       "  'organized': 0.005301347274662916,\n",
       "  'parallel': 0.005301347274662916,\n",
       "  'parlante': 0.005301347274662916,\n",
       "  'picard': 0.005301347274662916,\n",
       "  'piotr': 0.005301347274662916,\n",
       "  'put': 0.005301347274662916,\n",
       "  'python': 0.0,\n",
       "  'section': 0.010602694549325832,\n",
       "  'sections': 0.010602694549325832,\n",
       "  'set': 0.005301347274662916,\n",
       "  'share': 0.005301347274662916,\n",
       "  'special': 0.005301347274662916,\n",
       "  'started': 0.001956569545063125,\n",
       "  'starts': 0.005301347274662916,\n",
       "  'steve': 0.005301347274662916,\n",
       "  'strings': 0.00391313909012625,\n",
       "  'thanks': 0.010602694549325832,\n",
       "  'videos': 0.005301347274662916,\n",
       "  'working': 0.005301347274662916,\n",
       "  'written': 0.010602694549325832},\n",
       " 'doc2': {'avoiding': 0.00973716846366658,\n",
       "  'based': 0.00973716846366658,\n",
       "  'check': 0.00973716846366658,\n",
       "  'choosing': 0.00973716846366658,\n",
       "  'content': 0.00973716846366658,\n",
       "  'course': 0.0035936991644016578,\n",
       "  'covers': 0.00973716846366658,\n",
       "  'created': 0.0035936991644016578,\n",
       "  'days': 0.00973716846366658,\n",
       "  'developers': 0.00973716846366658,\n",
       "  'difficulty': 0.00973716846366658,\n",
       "  'exercises': 0.0035936991644016578,\n",
       "  'features': 0.00973716846366658,\n",
       "  'future': 0.00973716846366658,\n",
       "  'going': 0.00973716846366658,\n",
       "  'good': 0.00973716846366658,\n",
       "  'google': 0.0035936991644016578,\n",
       "  'introductory': 0.00973716846366658,\n",
       "  'keep': 0.00973716846366658,\n",
       "  'know': 0.0035936991644016578,\n",
       "  'learning': 0.0035936991644016578,\n",
       "  'material': 0.0035936991644016578,\n",
       "  'mentioned': 0.00973716846366658,\n",
       "  'much': 0.0035936991644016578,\n",
       "  'new': 0.00973716846366658,\n",
       "  'newer': 0.00973716846366658,\n",
       "  'news': 0.00973716846366658,\n",
       "  'offered': 0.00973716846366658,\n",
       "  'online': 0.00973716846366658,\n",
       "  'page': 0.00973716846366658,\n",
       "  'pick': 0.00973716846366658,\n",
       "  'post': 0.00973716846366658,\n",
       "  'python': 0.0,\n",
       "  'recognize': 0.00973716846366658,\n",
       "  'recommend': 0.00973716846366658,\n",
       "  'releases': 0.00973716846366658,\n",
       "  'relevant': 0.00973716846366658,\n",
       "  'setup': 0.00973716846366658,\n",
       "  'tried': 0.00973716846366658,\n",
       "  'tutorial': 0.00973716846366658,\n",
       "  'universal': 0.00973716846366658,\n",
       "  'version': 0.00973716846366658,\n",
       "  'want': 0.00973716846366658,\n",
       "  'welcome': 0.00973716846366658},\n",
       " 'doc3': {'adapted': 0.005075758028932579,\n",
       "  'afterwards': 0.005075758028932579,\n",
       "  'basics': 0.010151516057865158,\n",
       "  'begin': 0.005075758028932579,\n",
       "  'bit': 0.005075758028932579,\n",
       "  'brush': 0.005075758028932579,\n",
       "  'coded': 0.005075758028932579,\n",
       "  'coding': 0.0018733112665498004,\n",
       "  'come': 0.005075758028932579,\n",
       "  'concepts': 0.005075758028932579,\n",
       "  'conditions': 0.005075758028932579,\n",
       "  'course': 0.0074932450661992014,\n",
       "  'cut': 0.005075758028932579,\n",
       "  'data': 0.005075758028932579,\n",
       "  'development': 0.005075758028932579,\n",
       "  'discuss': 0.005075758028932579,\n",
       "  'dive': 0.005075758028932579,\n",
       "  'easiest': 0.005075758028932579,\n",
       "  'everything': 0.005075758028932579,\n",
       "  'excited': 0.005075758028932579,\n",
       "  'faster': 0.005075758028932579,\n",
       "  'file': 0.005075758028932579,\n",
       "  'finish': 0.005075758028932579,\n",
       "  'first': 0.0018733112665498004,\n",
       "  'friendly': 0.005075758028932579,\n",
       "  'fun': 0.005075758028932579,\n",
       "  'functions': 0.005075758028932579,\n",
       "  'get': 0.005619933799649401,\n",
       "  'getting': 0.005075758028932579,\n",
       "  'great': 0.005075758028932579,\n",
       "  'hope': 0.005075758028932579,\n",
       "  'incentives': 0.005075758028932579,\n",
       "  'know': 0.005619933799649401,\n",
       "  'language': 0.0018733112665498004,\n",
       "  'languages': 0.010151516057865158,\n",
       "  'large': 0.005075758028932579,\n",
       "  'learning': 0.005619933799649401,\n",
       "  'let': 0.010151516057865158,\n",
       "  'loops': 0.005075758028932579,\n",
       "  'major': 0.005075758028932579,\n",
       "  'manipulation': 0.005075758028932579,\n",
       "  'move': 0.005075758028932579,\n",
       "  'much': 0.0018733112665498004,\n",
       "  'need': 0.015227274086797736,\n",
       "  'overall': 0.005075758028932579,\n",
       "  'place': 0.005075758028932579,\n",
       "  'pro': 0.005075758028932579,\n",
       "  'programming': 0.005075758028932579,\n",
       "  'projects': 0.005075758028932579,\n",
       "  'python': 0.0,\n",
       "  'quick': 0.010151516057865158,\n",
       "  'right': 0.005075758028932579,\n",
       "  'shop': 0.005075758028932579,\n",
       "  'small': 0.005075758028932579,\n",
       "  'started': 0.005619933799649401,\n",
       "  'stop': 0.005075758028932579,\n",
       "  'strings': 0.0018733112665498004,\n",
       "  'time': 0.020303032115730316,\n",
       "  'types': 0.005075758028932579,\n",
       "  'use': 0.005075758028932579,\n",
       "  'variables': 0.005075758028932579,\n",
       "  'waiting': 0.005075758028932579,\n",
       "  'way': 0.005075758028932579,\n",
       "  'whiz': 0.005075758028932579,\n",
       "  'world': 0.005075758028932579,\n",
       "  'write': 0.005075758028932579}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf={}\n",
    "# print(set(new_token))\n",
    "for word in tokens_all_new.keys():\n",
    "    tfidf_doc={}\n",
    "    for term in set(tokens_all_new[word]):\n",
    "        tfidf_doc[term]= tf(term,tokens_all_new[word]) * use_words_idf[term] # calculate tfidf for each doc\n",
    "    tfidf[word]= tfidf_doc\n",
    "tfidf #=tf*log(N/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tfidf_dataframe = pd.DataFrame(tfidf).transpose()\n",
    "print(\"Non-normalization data frame\")\n",
    "tfidf_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best fully Weighted system=tfidf/(sigma tfidf^2)^1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to do cosine normalization a data dictionary\n",
    "def best_fully_norm(dic): # dic is distionary data structure\n",
    "    dic_norm={}\n",
    "    factor=1.0/(sum(dic.values())**2)\n",
    "    factors=math.sqrt(factor)\n",
    "    for k in dic:\n",
    "        dic_norm[k] = dic[k]*factors\n",
    "    return dic_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': {'antoine': 0.013886509921028588,\n",
       "  'attribution': 0.013886509921028588,\n",
       "  'class': 0.013886509921028588,\n",
       "  'code': 0.013886509921028588,\n",
       "  'coding': 0.005125097638586422,\n",
       "  'colleagues': 0.013886509921028588,\n",
       "  'commons': 0.013886509921028588,\n",
       "  'cox': 0.013886509921028588,\n",
       "  'created': 0.005125097638586422,\n",
       "  'creative': 0.013886509921028588,\n",
       "  'day': 0.041659529763085755,\n",
       "  'director': 0.013886509921028588,\n",
       "  'end': 0.013886509921028588,\n",
       "  'engedu': 0.013886509921028588,\n",
       "  'enjoy': 0.013886509921028588,\n",
       "  'enlightened': 0.013886509921028588,\n",
       "  'exercise': 0.027773019842057177,\n",
       "  'exercises': 0.005125097638586422,\n",
       "  'first': 0.010250195277172844,\n",
       "  'free': 0.013886509921028588,\n",
       "  'generosity': 0.013886509921028588,\n",
       "  'get': 0.010250195277172844,\n",
       "  'glassman': 0.013886509921028588,\n",
       "  'google': 0.020500390554345688,\n",
       "  'group': 0.013886509921028588,\n",
       "  'help': 0.013886509921028588,\n",
       "  'includes': 0.013886509921028588,\n",
       "  'installed': 0.013886509921028588,\n",
       "  'intensive': 0.013886509921028588,\n",
       "  'internet': 0.013886509921028588,\n",
       "  'introducing': 0.013886509921028588,\n",
       "  'introduction': 0.027773019842057177,\n",
       "  'john': 0.013886509921028588,\n",
       "  'johnson': 0.013886509921028588,\n",
       "  'language': 0.005125097638586422,\n",
       "  'leading': 0.013886509921028588,\n",
       "  'lecture': 0.013886509921028588,\n",
       "  'left': 0.013886509921028588,\n",
       "  'license': 0.013886509921028588,\n",
       "  'link': 0.013886509921028588,\n",
       "  'linked': 0.013886509921028588,\n",
       "  'machine': 0.013886509921028588,\n",
       "  'maggie': 0.013886509921028588,\n",
       "  'makes': 0.013886509921028588,\n",
       "  'material': 0.020500390554345688,\n",
       "  'materials': 0.027773019842057177,\n",
       "  'nick': 0.013886509921028588,\n",
       "  'organized': 0.013886509921028588,\n",
       "  'parallel': 0.013886509921028588,\n",
       "  'parlante': 0.013886509921028588,\n",
       "  'picard': 0.013886509921028588,\n",
       "  'piotr': 0.013886509921028588,\n",
       "  'put': 0.013886509921028588,\n",
       "  'python': 0.0,\n",
       "  'section': 0.027773019842057177,\n",
       "  'sections': 0.027773019842057177,\n",
       "  'set': 0.013886509921028588,\n",
       "  'share': 0.013886509921028588,\n",
       "  'special': 0.013886509921028588,\n",
       "  'started': 0.005125097638586422,\n",
       "  'starts': 0.013886509921028588,\n",
       "  'steve': 0.013886509921028588,\n",
       "  'strings': 0.010250195277172844,\n",
       "  'thanks': 0.027773019842057177,\n",
       "  'videos': 0.013886509921028588,\n",
       "  'working': 0.013886509921028588,\n",
       "  'written': 0.027773019842057177},\n",
       " 'doc2': {'avoiding': 0.026348682356485607,\n",
       "  'based': 0.026348682356485607,\n",
       "  'check': 0.026348682356485607,\n",
       "  'choosing': 0.026348682356485607,\n",
       "  'content': 0.026348682356485607,\n",
       "  'course': 0.009724514690375535,\n",
       "  'covers': 0.026348682356485607,\n",
       "  'created': 0.009724514690375535,\n",
       "  'days': 0.026348682356485607,\n",
       "  'developers': 0.026348682356485607,\n",
       "  'difficulty': 0.026348682356485607,\n",
       "  'exercises': 0.009724514690375535,\n",
       "  'features': 0.026348682356485607,\n",
       "  'future': 0.026348682356485607,\n",
       "  'going': 0.026348682356485607,\n",
       "  'good': 0.026348682356485607,\n",
       "  'google': 0.009724514690375535,\n",
       "  'introductory': 0.026348682356485607,\n",
       "  'keep': 0.026348682356485607,\n",
       "  'know': 0.009724514690375535,\n",
       "  'learning': 0.009724514690375535,\n",
       "  'material': 0.009724514690375535,\n",
       "  'mentioned': 0.026348682356485607,\n",
       "  'much': 0.009724514690375535,\n",
       "  'new': 0.026348682356485607,\n",
       "  'newer': 0.026348682356485607,\n",
       "  'news': 0.026348682356485607,\n",
       "  'offered': 0.026348682356485607,\n",
       "  'online': 0.026348682356485607,\n",
       "  'page': 0.026348682356485607,\n",
       "  'pick': 0.026348682356485607,\n",
       "  'post': 0.026348682356485607,\n",
       "  'python': 0.0,\n",
       "  'recognize': 0.026348682356485607,\n",
       "  'recommend': 0.026348682356485607,\n",
       "  'releases': 0.026348682356485607,\n",
       "  'relevant': 0.026348682356485607,\n",
       "  'setup': 0.026348682356485607,\n",
       "  'tried': 0.026348682356485607,\n",
       "  'tutorial': 0.026348682356485607,\n",
       "  'universal': 0.026348682356485607,\n",
       "  'version': 0.026348682356485607,\n",
       "  'want': 0.026348682356485607,\n",
       "  'welcome': 0.026348682356485607},\n",
       " 'doc3': {'adapted': 0.013937189928861097,\n",
       "  'afterwards': 0.013937189928861097,\n",
       "  'basics': 0.027874379857722193,\n",
       "  'begin': 0.013937189928861097,\n",
       "  'bit': 0.013937189928861097,\n",
       "  'brush': 0.013937189928861097,\n",
       "  'coded': 0.013937189928861097,\n",
       "  'coding': 0.005143802121566166,\n",
       "  'come': 0.013937189928861097,\n",
       "  'concepts': 0.013937189928861097,\n",
       "  'conditions': 0.013937189928861097,\n",
       "  'course': 0.020575208486264665,\n",
       "  'cut': 0.013937189928861097,\n",
       "  'data': 0.013937189928861097,\n",
       "  'development': 0.013937189928861097,\n",
       "  'discuss': 0.013937189928861097,\n",
       "  'dive': 0.013937189928861097,\n",
       "  'easiest': 0.013937189928861097,\n",
       "  'everything': 0.013937189928861097,\n",
       "  'excited': 0.013937189928861097,\n",
       "  'faster': 0.013937189928861097,\n",
       "  'file': 0.013937189928861097,\n",
       "  'finish': 0.013937189928861097,\n",
       "  'first': 0.005143802121566166,\n",
       "  'friendly': 0.013937189928861097,\n",
       "  'fun': 0.013937189928861097,\n",
       "  'functions': 0.013937189928861097,\n",
       "  'get': 0.015431406364698499,\n",
       "  'getting': 0.013937189928861097,\n",
       "  'great': 0.013937189928861097,\n",
       "  'hope': 0.013937189928861097,\n",
       "  'incentives': 0.013937189928861097,\n",
       "  'know': 0.015431406364698499,\n",
       "  'language': 0.005143802121566166,\n",
       "  'languages': 0.027874379857722193,\n",
       "  'large': 0.013937189928861097,\n",
       "  'learning': 0.015431406364698499,\n",
       "  'let': 0.027874379857722193,\n",
       "  'loops': 0.013937189928861097,\n",
       "  'major': 0.013937189928861097,\n",
       "  'manipulation': 0.013937189928861097,\n",
       "  'move': 0.013937189928861097,\n",
       "  'much': 0.005143802121566166,\n",
       "  'need': 0.041811569786583284,\n",
       "  'overall': 0.013937189928861097,\n",
       "  'place': 0.013937189928861097,\n",
       "  'pro': 0.013937189928861097,\n",
       "  'programming': 0.013937189928861097,\n",
       "  'projects': 0.013937189928861097,\n",
       "  'python': 0.0,\n",
       "  'quick': 0.027874379857722193,\n",
       "  'right': 0.013937189928861097,\n",
       "  'shop': 0.013937189928861097,\n",
       "  'small': 0.013937189928861097,\n",
       "  'started': 0.015431406364698499,\n",
       "  'stop': 0.013937189928861097,\n",
       "  'strings': 0.005143802121566166,\n",
       "  'time': 0.055748759715444386,\n",
       "  'types': 0.013937189928861097,\n",
       "  'use': 0.013937189928861097,\n",
       "  'variables': 0.013937189928861097,\n",
       "  'waiting': 0.013937189928861097,\n",
       "  'way': 0.013937189928861097,\n",
       "  'whiz': 0.013937189928861097,\n",
       "  'world': 0.013937189928861097,\n",
       "  'write': 0.013937189928861097}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate tfidf with cosine normalization\n",
    "tfidf_norm={} # declare tfidf dictionary to store tfidf value\n",
    "for word in tokens_all_new.keys():\n",
    "    tfidf_doc={} # delare tfidf_doc as a dictionary to store tfidf of each doc\n",
    "    for term in set(tokens_all_new[word]):\n",
    "        tfidf_doc[term]= tf(term,tokens_all_new[word]) * use_words_idf[term] # calculate tfidf for each doc\n",
    "    tfidf_norm[word]= best_fully_norm(tfidf_doc)\n",
    "tfidf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization data frame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapted</th>\n",
       "      <th>afterwards</th>\n",
       "      <th>antoine</th>\n",
       "      <th>attribution</th>\n",
       "      <th>avoiding</th>\n",
       "      <th>based</th>\n",
       "      <th>basics</th>\n",
       "      <th>begin</th>\n",
       "      <th>bit</th>\n",
       "      <th>brush</th>\n",
       "      <th>...</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>welcome</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027874</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adapted  afterwards   antoine  attribution  avoiding     based  \\\n",
       "doc1       NaN         NaN  0.013887     0.013887       NaN       NaN   \n",
       "doc2       NaN         NaN       NaN          NaN  0.026349  0.026349   \n",
       "doc3  0.013937    0.013937       NaN          NaN       NaN       NaN   \n",
       "\n",
       "        basics     begin       bit     brush    ...       videos   waiting  \\\n",
       "doc1       NaN       NaN       NaN       NaN    ...     0.013887       NaN   \n",
       "doc2       NaN       NaN       NaN       NaN    ...          NaN       NaN   \n",
       "doc3  0.027874  0.013937  0.013937  0.013937    ...          NaN  0.013937   \n",
       "\n",
       "          want       way   welcome      whiz   working     world     write  \\\n",
       "doc1       NaN       NaN       NaN       NaN  0.013887       NaN       NaN   \n",
       "doc2  0.026349       NaN  0.026349       NaN       NaN       NaN       NaN   \n",
       "doc3       NaN  0.013937       NaN  0.013937       NaN  0.013937  0.013937   \n",
       "\n",
       "       written  \n",
       "doc1  0.027773  \n",
       "doc2       NaN  \n",
       "doc3       NaN  \n",
       "\n",
       "[3 rows x 161 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf_dataframe = pd.DataFrame(tfidf_norm).transpose()\n",
    "print(\"Normalization data frame\")\n",
    "tfidf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Assume that the goal is to distinguish the differences between the three documents based on their features (term). Doing a feature selection based on POS in question 2 compared to not implementing a feature selection, do you think which option would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I think the method used in Question 2 is better, because if we do not base on POS, our outcome may have some adverb(RB) or numbers(CD), which will affect our conclusion of the features (terms) of these document. After using the POS, we can narrow down our range to only  Noun, Verb, and Adjective, and this will better help us distinguish the differences between the three documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) (Based on the content of the 3 document files above, do you suggest a better feature selection method? (use POS but choose other POS tags or a different method or combination of methods) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the contents of the 3 document, there are several names appear in the outcome, while we do not know how important it is. In this way, we can still use POS tags at the beginning, while then use chunking method after pos-tagging, for the pos-tagging method return to single vocabulary,such as\"Mary\",\"cat\", but sometimes what we need is the word phrase which is consisted by several single volcabulary, such as\"Mary's cat\". Therefore, in this way, we should use chunk method to get the information we want. Based on what I've discover, by only using the Pos-tagging cannot accurately distinguish the proper noun, for example, ('john', 'VB'), ('antoine', 'JJ'), \"john\"and \"antoine\"are both proper noun, while they are given different pos tag -- john is regarded as a verb, and the antoine is taken as an adjective. Thus, by using chunk, as it also has a standard chunking tags, we can distinguish the Noun phrase (NP) or Verb phrase(VP), so it is very important for people to extract some features from the documents which have many people's name or other proper noun, etc. We can use spacy or textboob which both provide some phrases or make up a grammer by using the nltk to help us further analysing the documents and get  better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
